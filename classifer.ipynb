{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8f926bd-3f5e-44a1-9ae4-334d2166e540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (2017, 17)\n",
      "\n",
      "First 5 rows:\n",
      "   Unnamed: 0  acousticness  danceability  duration_ms  energy  \\\n",
      "0           0        0.0102         0.833       204600   0.434   \n",
      "1           1        0.1990         0.743       326933   0.359   \n",
      "2           2        0.0344         0.838       185707   0.412   \n",
      "3           3        0.6040         0.494       199413   0.338   \n",
      "4           4        0.1800         0.678       392893   0.561   \n",
      "\n",
      "   instrumentalness  key  liveness  loudness  mode  speechiness    tempo  \\\n",
      "0          0.021900    2    0.1650    -8.795     1       0.4310  150.062   \n",
      "1          0.006110    1    0.1370   -10.401     1       0.0794  160.083   \n",
      "2          0.000234    2    0.1590    -7.148     1       0.2890   75.044   \n",
      "3          0.510000    5    0.0922   -15.236     1       0.0261   86.468   \n",
      "4          0.512000    5    0.4390   -11.648     0       0.0694  174.004   \n",
      "\n",
      "   time_signature  valence  target      song_title            artist  \n",
      "0               4    0.286       1        Mask Off            Future  \n",
      "1               4    0.588       1         Redbone  Childish Gambino  \n",
      "2               4    0.173       1    Xanny Family            Future  \n",
      "3               4    0.230       1  Master Of None       Beach House  \n",
      "4               4    0.904       1  Parallel Lines       Junior Boys  \n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2017 entries, 0 to 2016\n",
      "Data columns (total 17 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Unnamed: 0        2017 non-null   int64  \n",
      " 1   acousticness      2017 non-null   float64\n",
      " 2   danceability      2017 non-null   float64\n",
      " 3   duration_ms       2017 non-null   int64  \n",
      " 4   energy            2017 non-null   float64\n",
      " 5   instrumentalness  2017 non-null   float64\n",
      " 6   key               2017 non-null   int64  \n",
      " 7   liveness          2017 non-null   float64\n",
      " 8   loudness          2017 non-null   float64\n",
      " 9   mode              2017 non-null   int64  \n",
      " 10  speechiness       2017 non-null   float64\n",
      " 11  tempo             2017 non-null   float64\n",
      " 12  time_signature    2017 non-null   int64  \n",
      " 13  valence           2017 non-null   float64\n",
      " 14  target            2017 non-null   int64  \n",
      " 15  song_title        2017 non-null   object \n",
      " 16  artist            2017 non-null   object \n",
      "dtypes: float64(9), int64(6), object(2)\n",
      "memory usage: 268.0+ KB\n",
      "None\n",
      "\n",
      "Missing values in each column:\n",
      "Unnamed: 0          0\n",
      "acousticness        0\n",
      "danceability        0\n",
      "duration_ms         0\n",
      "energy              0\n",
      "instrumentalness    0\n",
      "key                 0\n",
      "liveness            0\n",
      "loudness            0\n",
      "mode                0\n",
      "speechiness         0\n",
      "tempo               0\n",
      "time_signature      0\n",
      "valence             0\n",
      "target              0\n",
      "song_title          0\n",
      "artist              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import libraries and load the dataset\n",
    "\n",
    "import pandas as pd  # For handling data\n",
    "import numpy as np   # For numerical operations\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"spotifydataset.csv\")\n",
    "\n",
    "# Show the shape of the dataset (rows, columns)\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "\n",
    "# Display the first 5 rows of the dataset\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Show info about each column: data type and non-null values\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())\n",
    "\n",
    "# Check if any column has missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "516fc181-160a-4fd2-b040-8dd3c11fb2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape (X): (2017, 13)\n",
      "Target shape (y): (2017,)\n",
      "\n",
      " Preview of features (X):\n",
      "   acousticness  danceability  duration_ms  energy  instrumentalness  key  \\\n",
      "0        0.0102         0.833       204600   0.434          0.021900    2   \n",
      "1        0.1990         0.743       326933   0.359          0.006110    1   \n",
      "2        0.0344         0.838       185707   0.412          0.000234    2   \n",
      "3        0.6040         0.494       199413   0.338          0.510000    5   \n",
      "4        0.1800         0.678       392893   0.561          0.512000    5   \n",
      "\n",
      "   liveness  loudness  mode  speechiness    tempo  time_signature  valence  \n",
      "0    0.1650    -8.795     1       0.4310  150.062               4    0.286  \n",
      "1    0.1370   -10.401     1       0.0794  160.083               4    0.588  \n",
      "2    0.1590    -7.148     1       0.2890   75.044               4    0.173  \n",
      "3    0.0922   -15.236     1       0.0261   86.468               4    0.230  \n",
      "4    0.4390   -11.648     0       0.0694  174.004               4    0.904  \n",
      "\n",
      " Preview of target (y):\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Select Features and Target for the Classifier\n",
    "\n",
    "# Define the list of feature columns we'll use to predict the song's popularity\n",
    "features = [\n",
    "    'acousticness', 'danceability', 'duration_ms', 'energy',\n",
    "    'instrumentalness', 'key', 'liveness', 'loudness', 'mode',\n",
    "    'speechiness', 'tempo', 'time_signature', 'valence'\n",
    "]\n",
    "\n",
    "# Create the input feature matrix X from the DataFrame using the selected features\n",
    "X = df[features]\n",
    "\n",
    "# Define the target variable y using the existing 'target' column\n",
    "# This column should contain 0 (not popular) or 1 (popular)\n",
    "y = df['target']\n",
    "\n",
    "# Print the shapes of X and y to confirm\n",
    "print(\"Features shape (X):\", X.shape)\n",
    "print(\"Target shape (y):\", y.shape)\n",
    "\n",
    "# Print a preview of the input features\n",
    "print(\"\\n Preview of features (X):\")\n",
    "print(X.head())\n",
    "\n",
    "# Print a preview of the target values\n",
    "print(\"\\n Preview of target (y):\")\n",
    "print(y.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a22f2c6-7e39-4c93-bd8e-5811eb9a35b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (1613, 13)\n",
      "Testing features shape: (404, 13)\n",
      "Training target shape: (1613,)\n",
      "Testing target shape: (404,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Show the shapes of the splits\n",
    "print(\"Training features shape:\", X_train.shape)\n",
    "print(\"Testing features shape:\", X_test.shape)\n",
    "print(\"Training target shape:\", y_train.shape)\n",
    "print(\"Testing target shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9303a8f1-fe79-4e1e-9f3e-d75b2d159da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Set: 0.77\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.76      0.77       206\n",
      "           1       0.76      0.78      0.77       198\n",
      "\n",
      "    accuracy                           0.77       404\n",
      "   macro avg       0.77      0.77      0.77       404\n",
      "weighted avg       0.77      0.77      0.77       404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy on Test Set: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a2dec1c-f265-4d64-aa2e-fc94a2d4422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict New Songs' Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04b2eeb3-159b-4b4d-82f1-2d6c3342df1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fca9a13-7275-4520-aa57-452f001181b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The song is likely to be NOT popular.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example new song's features (dummy values)\n",
    "# Make sure the order of features matches X.columns\n",
    "new_song = np.array([[0.35, 0.75, 210000, 0.55, 0.0, 5, 0.1, -6.0, 1, 0.04, 120.0, 4, 0.6]])\n",
    "\n",
    "# Predict popularity\n",
    "prediction = model.predict(new_song)\n",
    "\n",
    "if prediction[0] == 1:\n",
    "    print(\"The song is likely to be POPULAR!\")\n",
    "else:\n",
    "    print(\"The song is likely to be NOT popular.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8ca1553-9b50-4182-8bd4-43b931f4c5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test on Real Songs from Your Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc2c4e94-0074-406f-9438-00043beaae82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Subways - In Flagranti Extended Edit' by The Avalanches is likely to be POPULAR!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Pick a song row (e.g., row 10)\n",
    "song_features = df[features].iloc[10].values.reshape(1, -1)\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(song_features)\n",
    "title = df.iloc[10]['song_title']\n",
    "artist = df.iloc[10]['artist']\n",
    "\n",
    "if prediction[0] == 1:\n",
    "    print(f\"'{title}' by {artist} is likely to be POPULAR!\")\n",
    "else:\n",
    "    print(f\"'{title}' by {artist} is likely to be NOT popular.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90921d7b-9743-4a94-8bf5-5afa08981950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Feature  Importance\n",
      "4   instrumentalness    0.135026\n",
      "7           loudness    0.124215\n",
      "9        speechiness    0.103947\n",
      "3             energy    0.096407\n",
      "2        duration_ms    0.092485\n",
      "1       danceability    0.092418\n",
      "0       acousticness    0.090106\n",
      "12           valence    0.078812\n",
      "10             tempo    0.069444\n",
      "6           liveness    0.064945\n",
      "5                key    0.037040\n",
      "8               mode    0.010887\n",
      "11    time_signature    0.004267\n"
     ]
    }
   ],
   "source": [
    "#Improve Model Performance\n",
    "#Check Feature Importance\n",
    "import pandas as pd\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "print(feature_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f588ea19-6045-42bd-be4c-281cfc7fd973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retraining the data after finding out the important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a99e5bf2-1002-499f-b0d2-f96214077c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "             Feature  Importance\n",
      "0         Unnamed: 0    0.781442\n",
      "5   instrumentalness    0.048220\n",
      "8           loudness    0.031081\n",
      "1       acousticness    0.022988\n",
      "4             energy    0.021247\n",
      "10       speechiness    0.019680\n",
      "2       danceability    0.019432\n",
      "3        duration_ms    0.018254\n",
      "13           valence    0.012834\n",
      "11             tempo    0.010873\n",
      "7           liveness    0.007591\n",
      "6                key    0.004682\n",
      "9               mode    0.001175\n",
      "12    time_signature    0.000501\n",
      "\n",
      "Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       206\n",
      "           1       1.00      1.00      1.00       198\n",
      "\n",
      "    accuracy                           1.00       404\n",
      "   macro avg       1.00      1.00      1.00       404\n",
      "weighted avg       1.00      1.00      1.00       404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ----------------------\n",
    "# 1. Load dataset\n",
    "# ----------------------\n",
    "df = pd.read_csv(\"spotifydataset.csv\")\n",
    "df = df.dropna()\n",
    "\n",
    "# ----------------------\n",
    "# 2. Separate target and features\n",
    "# ----------------------\n",
    "target_col = \"target\"  # change this to your actual target column\n",
    "y = df[target_col]\n",
    "X = df.drop(columns=[target_col])\n",
    "\n",
    "# ----------------------\n",
    "# 3. Keep only numeric columns for ML\n",
    "# ----------------------\n",
    "X = X.select_dtypes(include=[\"number\"])\n",
    "\n",
    "# ----------------------\n",
    "# 4. Train-test split\n",
    "# ----------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# 5. Random Forest for feature importance\n",
    "# ----------------------\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "# ----------------------\n",
    "# 6. Retrain with top features\n",
    "# ----------------------\n",
    "top_n = 5\n",
    "top_features = feature_importance_df['Feature'].head(top_n)\n",
    "\n",
    "rf_top = RandomForestClassifier(random_state=42)\n",
    "rf_top.fit(X_train[top_features], y_train)\n",
    "\n",
    "y_pred = rf_top.predict(X_test[top_features])\n",
    "\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "120e4d4b-c396-4b9c-a7ed-3ab1858010df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance:\n",
      "              Feature  Importance\n",
      "0         Unnamed: 0    0.755383\n",
      "5   instrumentalness    0.047639\n",
      "8           loudness    0.038610\n",
      "2       danceability    0.024267\n",
      "3        duration_ms    0.021402\n",
      "1       acousticness    0.021355\n",
      "10       speechiness    0.020814\n",
      "4             energy    0.020673\n",
      "13           valence    0.013620\n",
      "15            artist    0.008904\n",
      "11             tempo    0.008728\n",
      "7           liveness    0.007559\n",
      "14        song_title    0.006256\n",
      "6                key    0.003341\n",
      "9               mode    0.001024\n",
      "12    time_signature    0.000426\n",
      "\n",
      "Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       206\n",
      "           1       1.00      1.00      1.00       198\n",
      "\n",
      "    accuracy                           1.00       404\n",
      "   macro avg       1.00      1.00      1.00       404\n",
      "weighted avg       1.00      1.00      1.00       404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# -----------------------\n",
    "# Load your dataset\n",
    "# -----------------------\n",
    "df = pd.read_csv(\"spotifydataset.csv\")  # change to your CSV name\n",
    "\n",
    "# -----------------------\n",
    "# Automatically encode non-numeric columns\n",
    "# -----------------------\n",
    "label_encoders = {}\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "# -----------------------\n",
    "# Define features and target\n",
    "# -----------------------\n",
    "X = df.drop(\"target\", axis=1)  # replace target_column with your target column name\n",
    "y = df[\"target\"]\n",
    "\n",
    "# -----------------------\n",
    "# Train-test split\n",
    "# -----------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Train model\n",
    "# -----------------------\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------\n",
    "# Feature importance\n",
    "# -----------------------\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": model.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\\n\", feature_importance)\n",
    "\n",
    "# -----------------------\n",
    "# Accuracy & classification report\n",
    "# -----------------------\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "385fd2c8-78eb-4f6d-ba1d-49866b6c7b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ID-like columns that leak info\n",
    "cols_to_drop = [\"Unnamed: 0\", \"song_title\", \"artist\"]  # you can add/remove as needed\n",
    "df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# Redo train-test split & training here...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce401739-0d85-4ead-8338-7b0a7408f140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       206\n",
      "           1       1.00      1.00      1.00       198\n",
      "\n",
      "    accuracy                           1.00       404\n",
      "   macro avg       1.00      1.00      1.00       404\n",
      "weighted avg       1.00      1.00      1.00       404\n",
      "\n",
      "\n",
      "Feature Importances:\n",
      " Unnamed: 0          0.781442\n",
      "instrumentalness    0.048220\n",
      "loudness            0.031081\n",
      "acousticness        0.022988\n",
      "energy              0.021247\n",
      "speechiness         0.019680\n",
      "danceability        0.019432\n",
      "duration_ms         0.018254\n",
      "valence             0.012834\n",
      "tempo               0.010873\n",
      "liveness            0.007591\n",
      "key                 0.004682\n",
      "mode                0.001175\n",
      "time_signature      0.000501\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"spotifydataset.csv\")\n",
    "\n",
    "# Drop non-numeric columns (optional if your data is already clean)\n",
    "df = df.select_dtypes(include=['number'])\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop(columns=['target'])  # replace with your actual target column name\n",
    "y = df['target']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize and train RandomForest\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importance\n",
    "importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "importances = importances.sort_values(ascending=False)\n",
    "print(\"\\nFeature Importances:\\n\", importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87b2fc5c-f2fb-4423-a5e3-f0742f902fd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Pleasure Power (DJ Smash Disco Remix)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30096\\1850956404.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# Predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1359\u001b[0m                 skip_parameter_validation=(\n\u001b[0;32m   1360\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m                 )\n\u001b[0;32m   1362\u001b[0m             ):\n\u001b[1;32m-> 1363\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m         X, y = validate_data(\n\u001b[0m\u001b[0;32m    360\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2967\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m\"estimator\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2968\u001b[0m                 \u001b[0mcheck_y_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2969\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2970\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2971\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2972\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2973\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2974\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ensure_2d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1364\u001b[0m         )\n\u001b[0;32m   1365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m     \u001b[0mensure_all_finite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deprecate_force_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1368\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m   1369\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m         \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1050\u001b[0m                         )\n\u001b[0;32m   1051\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1054\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1055\u001b[0m                 raise ValueError(\n\u001b[0;32m   1056\u001b[0m                     \u001b[1;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m                 ) from complex_warning\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;31m# Use NumPy API to support order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m         \u001b[1;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m         \u001b[1;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m   2164\u001b[0m             )\n\u001b[0;32m   2165\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2166\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2167\u001b[0m             \u001b[1;31m# Note: branch avoids `copy=None` for NumPy 1.x support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2168\u001b[1;33m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2169\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2170\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Pleasure Power (DJ Smash Disco Remix)'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"spotifydataset.csv\")\n",
    "\n",
    "# Drop leakage/index columns\n",
    "df = df.drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
    "\n",
    "# Features & target\n",
    "X = df.drop(columns=[\"target\"])\n",
    "y = df[\"target\"]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importances\n",
    "importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "print(\"\\nFeature Importances:\\n\", importances.sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf59b357-1e2c-4258-ae6e-6a35f99af9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"spotifydataset.csv\")\n",
    "\n",
    "# Drop leakage/index columns\n",
    "df = df.drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
    "\n",
    "# Features & target\n",
    "X = df.drop(columns=[\"target\"])\n",
    "y = df[\"target\"]\n",
    "\n",
    "# Keep only numeric columns\n",
    "X = X.select_dtypes(include=[\"number\"])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importances\n",
    "importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "print(\"\\nFeature Importances:\\n\", importances.sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0201f6-12d2-43ca-9da0-155b7d7eb1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load dataset\n",
    "file_path = (\"spotifydataset.csv\")\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "for col in [\"artist\", \"song_title\"]:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    label_encoders[col] = le  # store encoder if needed later\n",
    "\n",
    "# Define features & target\n",
    "X = data.drop(columns=[\"target\"])\n",
    "y = data[\"target\"]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# RandomForest with tuned parameters\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=300,       # more trees\n",
    "    max_depth=15,           # limit depth to prevent overfitting\n",
    "    min_samples_split=5,    # require more samples to split\n",
    "    min_samples_leaf=2,     # leaf size\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate accuracy\n",
    "train_acc = model.score(X_train, y_train)\n",
    "test_acc = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104f1118-8c60-4891-bed1-6372fdac17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"spotifydataset.csv\")\n",
    "\n",
    "# Target column\n",
    "target_col = \"target\"\n",
    "\n",
    "# Step 1: Detect leakage columns\n",
    "def find_leakage_columns(df, target_col, threshold=0.999):\n",
    "    leakage_cols = []\n",
    "    for col in df.columns:\n",
    "        if col != target_col and pd.api.types.is_numeric_dtype(df[col]):\n",
    "            corr = df[col].corr(df[target_col])\n",
    "            if abs(corr) >= threshold:  # suspiciously high correlation\n",
    "                leakage_cols.append(col)\n",
    "    return leakage_cols\n",
    "\n",
    "# Step 2: Detect ID-like columns\n",
    "def find_id_columns(df):\n",
    "    id_like_cols = []\n",
    "    for col in df.columns:\n",
    "        if df[col].nunique() == len(df):  # unique for every row\n",
    "            id_like_cols.append(col)\n",
    "        elif \"id\" in col.lower() or \"index\" in col.lower():\n",
    "            id_like_cols.append(col)\n",
    "    return id_like_cols\n",
    "\n",
    "# Find leakage & ID-like columns\n",
    "leakage_columns = find_leakage_columns(df, target_col)\n",
    "id_like_columns = find_id_columns(df)\n",
    "\n",
    "# Combine and drop duplicates\n",
    "columns_to_drop = list(set(leakage_columns + id_like_columns))\n",
    "\n",
    "print(\"🔍 Detected leakage columns:\", leakage_columns)\n",
    "print(\"🔍 Detected ID-like columns:\", id_like_columns)\n",
    "print(\"🗑 Dropping columns:\", columns_to_drop)\n",
    "\n",
    "# Drop leakage columns\n",
    "df_clean = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Step 3: Train-Test split\n",
    "X = df_clean.drop(columns=[target_col])\n",
    "y = df_clean[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Train model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "print(\"\\n✅ Training Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"✅ Testing Accuracy:\", accuracy_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9529205d-0878-4da7-a10a-730c76bd0659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"spotifydataset.csv\")\n",
    "\n",
    "target_col = \"target\"\n",
    "\n",
    "# Step 1: Detect leakage columns\n",
    "def find_leakage_columns(df, target_col, threshold=0.999):\n",
    "    leakage_cols = []\n",
    "    for col in df.columns:\n",
    "        if col != target_col and pd.api.types.is_numeric_dtype(df[col]):\n",
    "            corr = df[col].corr(df[target_col])\n",
    "            if abs(corr) >= threshold:  # suspiciously high correlation\n",
    "                leakage_cols.append(col)\n",
    "    return leakage_cols\n",
    "\n",
    "# Step 2: Detect ID-like columns\n",
    "def find_id_columns(df):\n",
    "    id_like_cols = []\n",
    "    for col in df.columns:\n",
    "        if df[col].nunique() == len(df):  # all unique\n",
    "            id_like_cols.append(col)\n",
    "        elif \"id\" in col.lower() or \"index\" in col.lower():\n",
    "            id_like_cols.append(col)\n",
    "    return id_like_cols\n",
    "\n",
    "# Step 3: Detect non-numeric columns (to drop or encode)\n",
    "def find_non_numeric_columns(df, target_col):\n",
    "    return [col for col in df.columns if col != target_col and not pd.api.types.is_numeric_dtype(df[col])]\n",
    "\n",
    "# Find problematic columns\n",
    "leakage_columns = find_leakage_columns(df, target_col)\n",
    "id_like_columns = find_id_columns(df)\n",
    "non_numeric_columns = find_non_numeric_columns(df, target_col)\n",
    "\n",
    "# Combine drop list\n",
    "columns_to_drop = list(set(leakage_columns + id_like_columns + non_numeric_columns))\n",
    "\n",
    "print(\"🔍 Detected leakage columns:\", leakage_columns)\n",
    "print(\"🔍 Detected ID-like columns:\", id_like_columns)\n",
    "print(\"🔍 Detected non-numeric columns:\", non_numeric_columns)\n",
    "print(\"🗑 Dropping columns:\", columns_to_drop)\n",
    "\n",
    "# Drop the problematic columns\n",
    "df_clean = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Train-test split\n",
    "X = df_clean.drop(columns=[target_col])\n",
    "y = df_clean[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "print(\"\\n✅ Training Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"✅ Testing Accuracy:\", accuracy_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35e1fc4-17d3-4ade-a3db-a4cacc38fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "362d8db2-746a-4c88-ae78-8b7843b8d555",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split, RandomizedSearchCV\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Assuming you already have X (features) and y (target)\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you already have X (features) and y (target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(eval_metric='mlogloss', use_label_encoder=False)\n",
    "\n",
    "# Parameter grid for tuning\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(50, 500, 50),           # Number of trees\n",
    "    'max_depth': np.arange(2, 12, 1),                 # Tree depth\n",
    "    'learning_rate': np.linspace(0.01, 0.3, 10),      # Step size shrinkage\n",
    "    'subsample': np.linspace(0.6, 1.0, 5),            # Fraction of samples\n",
    "    'colsample_bytree': np.linspace(0.6, 1.0, 5),     # Fraction of features\n",
    "    'gamma': np.linspace(0, 5, 10),                   # Minimum loss reduction\n",
    "    'reg_lambda': np.linspace(0.1, 5, 10),            # L2 regularization\n",
    "    'reg_alpha': np.linspace(0, 5, 10)                # L1 regularization\n",
    "}\n",
    "\n",
    "# Randomized Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,                # Number of random combinations\n",
    "    scoring='accuracy',\n",
    "    cv=5,                     # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the search\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "# Evaluate\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ec44a12-c6a8-456c-b2e5-1c61af2cc6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\dell\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ebb7878-568e-4587-bb81-f02ff8419f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3efa7de0-7cab-4d2a-ad9a-274d45afe59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 250 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n250 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\data.py\", line 407, in pandas_feature_info\n    new_feature_types.append(_pandas_dtype_mapper[dtype.name])\nKeyError: 'object'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n    return func(**kwargs)\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\sklearn.py\", line 1664, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\sklearn.py\", line 628, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\sklearn.py\", line 1137, in _create_dmatrix\n    return QuantileDMatrix(\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n    return func(**kwargs)\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\core.py\", line 1614, in __init__\n    self._init(\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\core.py\", line 1678, in _init\n    it.reraise()\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\core.py\", line 572, in reraise\n    raise exc  # pylint: disable=raising-bad-type\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\core.py\", line 553, in _handle_exception\n    return fn()\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\core.py\", line 640, in <lambda>\n    return self._handle_exception(lambda: int(self.next(input_data)), 0)\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\data.py\", line 1654, in next\n    input_data(**self.kwargs)\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n    return func(**kwargs)\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\core.py\", line 620, in input_data\n    new, cat_codes, feature_names, feature_types = _proxy_transform(\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\data.py\", line 1707, in _proxy_transform\n    df, feature_names, feature_types = _transform_pandas_df(\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\data.py\", line 640, in _transform_pandas_df\n    feature_names, feature_types = pandas_feature_info(\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\data.py\", line 409, in pandas_feature_info\n    _invalid_dataframe_dtype(data)\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\data.py\", line 372, in _invalid_dataframe_dtype\n    raise ValueError(msg)\nValueError: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:song_title: object, artist: object\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 39\u001b[0m\n\u001b[0;32m     27\u001b[0m random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m     28\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mxgb_model,\n\u001b[0;32m     29\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mparam_dist,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Fit the search\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[43mrandom_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Best parameters\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\base.py:1363\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1359\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1360\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1361\u001b[0m     )\n\u001b[0;32m   1362\u001b[0m ):\n\u001b[1;32m-> 1363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1046\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1047\u001b[0m     )\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1051\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1992\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1990\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1991\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1992\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1994\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1995\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1996\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1028\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1024\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1025\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m   1026\u001b[0m     )\n\u001b[1;32m-> 1028\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:505\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    499\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    501\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    502\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m     )\n\u001b[1;32m--> 505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    509\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    510\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    515\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 250 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n250 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\data.py\", line 407, in pandas_feature_info\n    new_feature_types.append(_pandas_dtype_mapper[dtype.name])\nKeyError: 'object'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n    return func(**kwargs)\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\sklearn.py\", line 1664, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\sklearn.py\", line 628, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\sklearn.py\", line 1137, in _create_dmatrix\n    return QuantileDMatrix(\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n    return func(**kwargs)\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\core.py\", line 1614, in __init__\n    self._init(\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\core.py\", line 1678, in _init\n    it.reraise()\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\core.py\", line 572, in reraise\n    raise exc  # pylint: disable=raising-bad-type\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\core.py\", line 553, in _handle_exception\n    return fn()\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\core.py\", line 640, in <lambda>\n    return self._handle_exception(lambda: int(self.next(input_data)), 0)\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\data.py\", line 1654, in next\n    input_data(**self.kwargs)\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n    return func(**kwargs)\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\core.py\", line 620, in input_data\n    new, cat_codes, feature_names, feature_types = _proxy_transform(\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\data.py\", line 1707, in _proxy_transform\n    df, feature_names, feature_types = _transform_pandas_df(\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\data.py\", line 640, in _transform_pandas_df\n    feature_names, feature_types = pandas_feature_info(\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\data.py\", line 409, in pandas_feature_info\n    _invalid_dataframe_dtype(data)\n  File \"C:\\Users\\Dell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\data.py\", line 372, in _invalid_dataframe_dtype\n    raise ValueError(msg)\nValueError: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:song_title: object, artist: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you already have X (features) and y (target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(eval_metric='mlogloss', use_label_encoder=False)\n",
    "\n",
    "# Parameter grid for tuning\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(50, 500, 50),           # Number of trees\n",
    "    'max_depth': np.arange(2, 12, 1),                 # Tree depth\n",
    "    'learning_rate': np.linspace(0.01, 0.3, 10),      # Step size shrinkage\n",
    "    'subsample': np.linspace(0.6, 1.0, 5),            # Fraction of samples\n",
    "    'colsample_bytree': np.linspace(0.6, 1.0, 5),     # Fraction of features\n",
    "    'gamma': np.linspace(0, 5, 10),                   # Minimum loss reduction\n",
    "    'reg_lambda': np.linspace(0.1, 5, 10),            # L2 regularization\n",
    "    'reg_alpha': np.linspace(0, 5, 10)                # L1 regularization\n",
    "}\n",
    "\n",
    "# Randomized Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,                # Number of random combinations\n",
    "    scoring='accuracy',\n",
    "    cv=5,                     # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the search\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "# Evaluate\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cffe14e5-23ed-4903-9c91-3bbf80ad716b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑 Dropping columns: ['artist', 'Unnamed: 0', 'song_title']\n",
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
      "🎯 Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "✅ Training Accuracy: 0.9616\n",
      "✅ Testing Accuracy: 0.7723\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ===== Load Dataset =====\n",
    "df = pd.read_csv(\"spotifydataset.csv\")\n",
    "\n",
    "# ===== Detect Leakage Columns =====\n",
    "def detect_leakage(df):\n",
    "    leakage_cols = []\n",
    "    id_like_cols = []\n",
    "    non_numeric_cols = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].nunique() == df.shape[0]:  # Unique values = rows → ID-like\n",
    "            id_like_cols.append(col)\n",
    "        elif any(keyword in col.lower() for keyword in [\"id\", \"uuid\", \"number\"]):\n",
    "            id_like_cols.append(col)\n",
    "\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            non_numeric_cols.append(col)\n",
    "\n",
    "    return leakage_cols, id_like_cols, non_numeric_cols\n",
    "\n",
    "leakage_cols, id_like_cols, non_numeric_cols = detect_leakage(df)\n",
    "\n",
    "# Columns to drop\n",
    "drop_cols = list(set(leakage_cols + id_like_cols + non_numeric_cols))\n",
    "print(f\"🗑 Dropping columns: {drop_cols}\")\n",
    "\n",
    "df_clean = df.drop(columns=drop_cols)\n",
    "\n",
    "# ===== Features & Target =====\n",
    "X = df_clean.drop(columns=[\"target\"])  # Change 'target' to your label column\n",
    "y = df_clean[\"target\"]\n",
    "\n",
    "# ===== Train-Test Split =====\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ===== Model & Hyperparameter Tuning =====\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.3],\n",
    "    \"subsample\": [0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(eval_metric=\"mlogloss\", random_state=42)\n",
    "grid_search = GridSearchCV(xgb, param_grid, cv=3, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ===== Best Params & Accuracy =====\n",
    "print(\"🎯 Best Parameters:\", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "print(f\"✅ Training Accuracy: {accuracy_score(y_train, y_pred_train):.4f}\")\n",
    "print(f\"✅ Testing Accuracy: {accuracy_score(y_test, y_pred_test):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca231e2f-b404-4c8a-8628-ff5618ccea8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
